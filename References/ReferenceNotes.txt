---------------------
Primary References
---------------------

Contractive Auto-Encoders: Explicit Invariance During Feature Extraction
http://techtalks.tv/talks/contractive-auto-encoders-explicit-invariance-during-feature-extraction/54426/
http://www.icml-2011.org/papers/455_icmlpaper.pdf

- A good description is that the term is the Schatten Norm of the Representation of the CAE not the Schatten Norm of the Reconstruction

Adam's Notes
- Explanation of Jacobian portion at 4:00
- Want same representation for small changes to the training points through an invariant hidden layer.
- Lower jacobian norm correlates with better generalization error
- Possibly compare to stacked contractive auto-encoders. Invariance increases as more stacking. Further radii map to the same representation.

Higher Order Contractive Auto-Encoder
http://videolectures.net/ecmlpkdd2011_rifai_contractive/
http://www.iro.umontreal.ca/~vincentp/Publications/CAE_H_ECML2011.pdf

Adam's Notes
- Auto-encoder is a encoder and a decoder.
- learns efficient representations by trying to reconstruct data
- Train an AE using a cost function which is the reconstruction mean square error
- 1st Order CAE: Frobenius norm of the Jacobian of the Hidden Layer (weights)
- For Higher Order CAE, use stochastic approximation of the Hessian Frobenius norm
- Want invariance in the hidden layer. Robust to small changes in input space
- Want locality around the training puts. Not the whole space.
- After 50 singular values, all the directions have contracted
- Estimation of the Global Input Space: average the ratio of the distance between sample points and distance between points in feature space as a ratio of the radius.
- Reconstruction error wants to expand in different directions and the regularization term tries to contract the directions
- By looking at the directions and magnitudes of the singular values you get the local tangent (local dimensionality) of the manifold
- local charts and atlas using first few singular values of the Jacobian helps to define the manifold
- Data Sets: Writers Corpus, CIFAR-10, and MNIST, rot, bg-img, rect

Manifold Tangent Classifier (uses Contractive Auto-Encoders)
http://videolectures.net/nips2011_dauphin_manifold/

---------------------
Secondary References
---------------------

Efficient sparse coding algorithms
http://ai.stanford.edu/~hllee/nips06-sparsecoding.pdf
- Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it learns basis functions that capture higher-level features in the data.
- The goal of sparse coding is to represent input vectors approximately as a weighted linear combination of a small number of (unknown) "basis vectors."
- In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem.

Learning invariant features through local space contraction (Rifai et al. 2011)
http://arxiv.org/pdf/1104.4153.pdf
http://www.iro.umontreal.ca/~lisa/pointeurs/CAE_tech_report.pdf

Learning Invariant Features through Topographic Filter Maps
http://yann.lecun.com/exdb/publis/pdf/koray-cvpr-09.pdf
- Pulled directly from the paper: It is well established that sparse coding algorithms applied to natural images learn basis functions that are localized oriented edges and resemble the receptive fields of simple cells in area V1 of the mammalian visual cortex [19]. These methods produce feature representation that are sparse, but not invariant. If the input pattern is slightly distorted, the representation may change drastically. Moreover, these features represent information about local texture, and hence, are rather inefficient when used to preprocess whole images because they do not exploit the redundancy in adjacent image patches. Finally, most sparse coding algorithms [19, 14, 17, 24, 4] have found limited applications in vision due to the high computational cost of the iterative optimization required to compute the feature descriptor.
- Automatically learn locally-invariant feature descriptors from in an unsupervised manner. Sparse coding can be applied to natural images to learn basis functions that are localized oriented edges. Introduces Invariant Predictive Sparse Decomposition learns invariance and produces efficient representations of data.

Reducing the Dimensionality of Data with Neural Networks
http://www.cs.toronto.edu/~hinton/science.pdf

---------------------
Other Resources
---------------------

http://stats.stackexchange.com/questions/14827/how-to-calculate-derivative-of-the-contractive-auto-encoder-regularization-term

GitHub CAE code by ynd
https://github.com/ynd/cae.py