A:	Started adding notes to paper (05/02/2012). Further work on notes/paper (05/04/2012)
A:  Fixed memory error issue in cae.py. Added cae_classifier.py which is a KNN for the trained CAE. (04/30/2012)
A:	Added cae_contraction.py which uses "results" from main.py and looks at variations in features around a single data point to see if these variations contract to the same encoding as the training point. It measures the difference between the encoding of the data point and the variation. I ran this a few times during training and a lower reconstruction seems to correlate with a lower difference between encodings. (04/28/2012)
A:	I updated some of the code and pulled a function from main.py into helper_functions.py. I am planning to make a file called cae_contraction.py where we can study the contraction of the CAE. I would think that it may be interesting to visualize the contraction of variations in a single training point to the same reconstruction. (04/27/2012)
A:	I removed Jiaggi2010.pdf from the References. The paper deals more with using the Nuclear Norm as a constraint rather than in the Objective function. I added Rifai2011-NIPS-MTC.pdf. It is a paper about the Manifold Tangent Classifier which is built on CAE. I renamed the other papers to denote their conference and what they are covering. I updated the outline and reference notes for related works. (04/26/2012)
A:  I made some changes to cae.py to use the Schatten Norm. There are two lines in cae.py you can use to see how close the Schatten Norm (p=2) approximates the Frobenius norm. Mathematically, they should be equal but the I think Numpy makes an approximation of the singular values during decomposition so we run into the differences in values. For more information, see http://scicomp.stackexchange.com/questions/1861/understanding-how-numpy-does-svd
The Schatten Norm has the additional benefit that we don't run into as many memory errors. I have been able to use about 1750 training points and 1024 hidden units.
I downloaded the NIPS LaTeX template files and started a very basic outline for the paper. Feel free to expand these as you see fit. (04/25/2012 11:30PM)
A: Computational Considerations: When using the program, you will need to drop the number of hidden units or else you may run out of memory. On a sample size of 100, I was getting a memory error. This is happens because the jacobian contains the W matrix, a matrix that is the number of hidden units by the number of samples. (04/23/2012 10:30PM)
A: https://github.com/ynd belongs to Yann N. Dauphin, one of the authors in papers we are referencing. One of his repositories has python code for the Contractive Autoencoder they present in the papers. I made a program that uses their code and runs it on MNIST, one of the datasets they use. To test the program, you will have to download the dataset at http://www.iro.umontreal.ca/~lisa/icml2007data/mnist.zip (unpacked = 750mb) and necessary libraries. Numpy (http://numpy.scipy.org/) and matplotlib (http://matplotlib.sourceforge.net/). Specify the path to the training program in main.py. Right now, I am using my laptop so I have it sampling the first 10 training images and then attempting to reconstruct the first image after training so it doesn't perform that well. Feel free to change this. (04/22/2012  11:30AM)
A: Files and Directories are set up (04/21/2012 1:42PM)
A: Testing and setting up GIT (04/21/2012 1:36PM)