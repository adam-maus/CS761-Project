\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09


\title{Regularization of Contractive Autoencoders using the Schatten Norm}

\author{
Adam Maus \\ 
Department of Computer Sciences \\
University of Wisconsin - Madison \\
Madison, WI 53706 \\
\texttt{maus@cs.wisc.edu} \\
\And
Brian Nixon \\
Department of Computer Sciences \\
University of Wisconsin - Madison \\
Madison, WI 53706 \\
\texttt{nixon@cs.wisc.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Contractive autoencoders have been used in unsupervised learning to learn a useful representation of data that minimizes reconstruction error and captures manifold structure through regularization by the Frobenius norm. We study a more general contractive autoencoder by replacing the Frobenius norm with the Schatten norm. The Schatten norm is a matrix norm that is equivalent to the $p$-norm of the singular values of a matrix while the Forbenius norm is a special case of the Schatten Norm with $p$ = 2. We show that using this penalty with $p$ = 1 results in greater contraction in the hidden layer of the autoencoder but tends to produce less accurate reconstruction.
\end{abstract}

\section{Introduction}

Algorithms that learn useful low-dimensional representations of high-dimensional data automatically have recently garnered attention because of their application in deep learning [Bengio 2009], manifold learning [Tenenbaum 2000, Roweis 2000], initialization of state-of-art classifiers [Bengio 2007], Deep Belief Networks [Hinton 2006 - Neural Comp], and Deep Boltzmann Machines [Salakhutdinov 2009]. 

Autoencoders are multilayer neural networks that can be used to learn low-dimensional codes of data [Hinton 1995, Hinton 2006]. Autoencoder applications include hand-written character and image recognition [Tan 2010], novelty assessment [Thompson 2002], and data visualization [Nadjarpoorsiyahkaly 2011]. Other types of autoencoders have been introduced that focus on smaller weights and robustness to noise. To encourage smaller weights and weight decay, a penalty term is added to the loss function that requires the sum of the weight values to be as small as possible [Kavukcuoglu 2009, Lee 2008]. Denoising autoencoders encourage invariance by first corrupting the input with noise and then trying to learn the uncorrupted form [Vincent 2010]. 

The traditional autoencoder with $h$ hidden nodes trained on $n$ points with $d$ dimensions has the following encoding function, $e(x) = f_1(Wx + b_1) $ and decoding function, $d(x) = f_2(W^{T} e(x) + b_2) $, where the activation functions $f_1(x)$, $f_2(x)$ take the form of the sigmoid function $\frac{1}{1 + e^{-x}}$ in this study. $f_2(x)$ may also be a linear activation function depending on the application. $W$ is a matrix of real numbers of size $h$ x $d$, $b_1$ and $b_2$ are bias vectors of real numbers of size $h$ and $d$, respectively.

\section{Contractive Autoencoders (CAE)}

Contractive autoencoders introduced by Rifai et al. [Rifai 2011] are similar in spirit to autoencoders with weight decay. The contractive autoencoder objective function is $L(W, b_1, b_2) = \sum^n_i{ || x_i - d(e(x_i)) ||^2 } + \lambda || J_e (x_i) ||_{S_p} $ where $|| Y ||_{S_p}$ represents the Schatten norm of $Y$ with a value of $p$. Rifai et al. use the Schatten norm with $p=2$ also known as the Frobenius norm of the Jacobian of the nonlinear encoding function: $ J_{e}(x_i) = \partial e(x_i) / \partial x_i$. Through this regularization, contractive autoencoders encourage a sparse represenation of the data that is also locally invariant. The goal of CAEs is to minimize the reconstruction error as well as the contraction around each of the training points. The contraction is maximized in directions orthogonal to the manifold so representations change very little in these directions while parallel directions have the most change in representation. 

CAEs learn deep representations of the data that are also robust to variations in the data by using an overcomplete representation and including the Jacobian penalty term. An overcomplete representation can be illustrated by choosing more hidden nodes than dimensions allowing an autoencoder multiple complete and perfect reconstructions. In a CAE, the additional Jacobian penalty term chooses reconstructions that are robust to small changes to the data and learns the manifold's tangent directions. It is through this overcomplete representation that a deeper understanding of the data can be found than in a traditional autoencoder. It should also be noted, that the jacobian penalty term is not limited to autoencoders and other models could benefit by minimizing the contractive penalty term.



\subsection{Schatten Norm}

\section{Experiments}

Training contractive autoencoders involves minimizing the $L(W, b_1, b_2)$. For these results, a gradient descent method was used to train the weights, $W$, $b_1$, and $b_2$.

We compare the performance of the CAE with varying $p$ using the basic MNIST dataset. After training the CAE on the first 400 samples taken from the dataset, the CAE was tested and compared by using a $k$ nearest neighbors classifier on the MNIST test data. The classifier was set up such that the trained CAE encoded each training point and each test point. For each encoded test point, the euclidean distance was computed between it and each training point. The $k$ closest neighbors voted on the label for the test point with ties were broken randomly using a uniform distribution.



\section{Conclusions}

%%%%%%%%%%%%%%%%%%%%

\subsection{Headings: second level}

Second level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be numbered consecutively. The corresponding
number is to appear enclosed in square brackets, such as [1] or [2]-[5]. The
corresponding references are to be listed in the same order at the end of the
paper, in the \textbf{References} section. (Note: the standard
\textsc{Bib\TeX} style \texttt{unsrt} produces this.) As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

As submission is double blind, refer to your own published work in the 
third person. That is, use ``In the previous work of Jones et al.\ [4]'',
not ``In our previous work [4]''. If you cite your other papers that
are not widely available (e.g.\ a journal paper under review), use
anonymous author names in the citation, e.g.\ an author of the
form ``A.\ Anonymous''. 


\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures. 
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include 
acknowledgments in the anonymized submission, only in the 
final paper. 

\subsubsection*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
