"The report length is 3 pages in NIPS style, excluding references but including images. You can have as many pages of references as you want." (Piazza)
* means the snippet of text is in the paper somehow

Regularization of contractive autoencoders using the Schatten norm

Adam Maus (maus@cs.wisc.edu)
Brian Nixon (nixon@cs.wisc.edu)

--------------
Abstract
--------------
* Contractive autoencoders have been used in unsupervised learning to learn a useful representation of data that minimizes reconstruction error and captures manifold structure through regularization by the Frobenius norm. We study a more general contractive autoencoder by replacing the Frobenius norm with the Schatten norm. The Schatten norm is a matrix norm that is equivalent to the $p$-norm of the singular values of a matrix while the Forbenius norm is a special case of the Schatten norm with $p$ = 2. We show that using this penalty with $p$ = 1 results in greater contraction in the hidden layer of the autoencoder but tends to produce less accurate reconstruction. (we need to actually show this)

--------------
Introduction
--------------
* Algorithms that learn useful low-dimensional representations of high-dimensional data automatically have recently garnered attention because of their application in deep learning [Bengio 2009], manifold learning [Tenenbaum 2000, Roweis 2000], initialization of state-of-art classifiers [Bengio 2007], Deep Belief Networks [Hinton 2006 - Neural Comp], and Deep Boltzmann Machines [Salakhutdinov 2009]. 

Dimensionality reduction through manifold learning has involved manifold charting [Brand 2003], sparse coding [Lee 2007, Kavukcuoglu 2009], local coordinate coding [Yu 2009, Yu 2010], nonlinear PCA [Scholz 2005], and autoencoders [Hinton 2006].

* Autoencoders are multilayer neural networks that can be used to learn low-dimensional codes of data [Hinton 1995, Hinton 2006]. Autoencoder applications include hand-written character and image recognition [Tan 2010], novelty assessment [Thompson 2002], and data visualization [Nadjarpoorsiyahkaly 2011]. Other types of autoencoders have been introduced the focus on smaller weights and robustness to noise. To encourage smaller weights and weight decay, a penalty term is added to the loss function that requires the sum of the weight values to be as small as possible [Kavukcuoglu 2009, Lee 2008]. Denoising autoencoders encourage invariance by first corrupting the input with noise and then try to learn the uncorrupted form [Vincent 2010]. 

* Contractive autoencoders introduced by Rifai et al. [Rifai 2011] are similar in spirit to autoencoders with weight decay. The autoencoder's objective function has the reconstruction error and an added penalty term, the Frobenius norm of the Jacobian of the training points and weights of the autoencoder. Through this regularization, contractive autoencoders encourage a sparse represenation of the data that is also locally invariant and lacks a preference for particular directions. This autoencoder learns efficient representations of the data that are robust to variations in the data. 

We introduce an autoencoder that replaces the Frobenius norm with the more general Schatten norm. The Schatten norm is a matrix norm that consists of the p-norm of the singular values of a matrix. The Schatten norm with p = 1 yields the nuclear norm, the Frobenius norm is equivalent to the Schatten norm when p = 2, and p = infinity is called the spectral norm. The penalty term consists of the average of the Schatten norm of the Jacobians of the autoencoder's weights on a training set. 

* The goal of contractive autoencoders is to minimize the reconstruction error as well as the contraction around each of the training points. The contraction is maximized in directions orthogonal to the manifold so representations change very little in these directions while parallel directions have the most change in representation. 

The largest singular values of the Jacobian correspond to the directions in input space to which the representation is most sensitive.

Once the autoencoder has learned a new representation of the data, a coordinate system can be produced by the hidden layer. Each data point maps to a particular low dimensional representation and represents a chart of the manifold at that point. In a contractive autoencoder with a minimized objective function, variations in the data map to the same representation and chart. These charts can be put together to form an atlas of the manifold and a global coordinate system.

The runtime computational cost of calculating the Schatten norm is relatively inexpensive and limited by the complexity of singular value decomposition of the Jacobian for each weight and training point. The space complexity is expensive since the Jacobian for each training point must be calculated and used in the singular value decomposition. The size of the Jacobian for a single training point is equal to the number of hidden nodes multiplied by the number of inputs. For the results on the MNIST dataset, the size of the Jacobian for one training example is 1000 by 784 or 784,000 entries.

Stacking autoencoders generally leads to better results, the encoded inputs from the first autoencoder leads directly into the next autoencoder yielding increased levels of representation.

* Training contractive autoencoders involves minimizing the reconstruction error and penalty term. For these results, a gradient descent method was used to train the weights, $W$, $b_1$, and $b_2$.

Local tangent directions of the manifold are defined by the Jacobian of the encoding function of the autoencoder. Other work has looked at the local curvature of the manifold using the Hessian of the encoding function.

The coordinate system defined by the cae can be characterized by the saturation of the hidden nodes. For example, moving in one direction along the manifold will result in hidden nodes that saturate. Similar to a group of overlapping circles in 2d space. The boundaries between circles represent hidden nodes saturating in that direction. In the Schatten norm with p = 1, this contraction will be stronger and the circles smaller.

When regularizing with the schatten norm using p=infinity, we are trying to minimize contraction in the maximal direction.  

* An overcomplete representation illustrated by choosing more hidden nodes than inputs in an autoencoder would allow multiple complete and perfect reconstructions. In a CAE, the additional Jacobian penalty term chooses reconstructions that are robust to small changes to the data and learns the manifold's tangent directions. It is through this overcomplete representation that a deeper understanding of the data can be found than in a traditional autoencoder. It should also be noted, that the jacobian penalty term is not limited to autoencoders and other models could benefit by minimizing the contractive penalty term.

--------------
Related Work
--------------

Manifold Charting
* Brand2003: Charting Manifolds by soft partitioning the data into locally linear low-dimensional neighborhoods before computing the connection that gives the global low dimensional embedding using a Gaussian Mixture Model

Sparse Coding
* Kavukcuoglu2009: Learning Invariant Features through Topographic Filter Maps: Automatically learn locally-invariant feature descriptors from in an unsupervised manner. Sparse coding can be applied to natural images to learn basis functions that are localized oriented edges. Introduces Invariant Predictive Sparse Decomposition learns invariance and produces efficient representations of data.
* Lee2007: Efficient sparse coding algorithms: Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given
only unlabeled input data, it learns basis functions that capture higher-level features in the data. Efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem.

Local Coordinate Coding
* Yu2009: Nonlinear Learning using Local Coordinate Coding: The manifold is locally approximated by a linear combination of a data point x's nearby anchor points. The linear weights are the local coordinate coding. Locally embed points on a manifold for a lower dimensional space and expressed as coordinates with respect to a set of anchor points. A nonlinear function is approximated by a linear function.
* Yu2010: Improved Local Coordinate Coding using Local Tangents: Extends Local Coordinate Coding by including local tangent directions through quadratic approximation terms.

Nonlinear Dimensionality Reduction using Neural Networks (Autoencoders)
* Hinton2006: Reducing the dimensionality of data with neural networks: Introduces autoencoder 

Denoising Autoencoders
* Vincent2010

Autoencoders with Weight Decay
* Lee2008

Other Manifold Learning methods see: http://www.postech.ac.kr/~seungjin/courses/dr/handouts/list.pdf
--------------
Contractive autoencoders
--------------
- Used to encode training and test points for kNN

--------------
Schatten Norm (if we have enough material)
--------------
Computational Complexity (both runtime and space)
- Numpy's SVD uses GESDD from LAPACK. http://scicomp.stackexchange.com/questions/1861/understanding-how-numpy-does-svd

--------------
Experiments 
--------------
Based off of Rifai2011-ICML-CAE
1) 	Table with following headers:
	Data Set, CAE (frobenius norm), CAE (schatten norm p=1), CAE (schatten norm p=inf), standard kNN
	Data Sets: basic
2)	Average contraction for a given radius as reconstruction error is minimized
3)	Average singular values for the CAE with varying p.
----

After the autoencoder has learned a new representation of the data, a classifier can be designed to use this new representation such as $k$ nearest neighbors in Table xx.

--------------
Conclusions
--------------